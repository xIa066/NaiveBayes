{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2019 Semester 1\n",
    "-----\n",
    "## Project 1: Gaining Information about Naive Bayes\n",
    "-----\n",
    "###### Student Name(s):\n",
    "###### Python version:\n",
    "###### Submission deadline: 1pm, Fri 5 Apr 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This iPython notebook is a template which you may use for your Project 1 submission. (You are not required to use it; in particular, there is no need to use iPython if you do not like it.)\n",
    "\n",
    "Marking will be applied on the five functions that are defined in this notebook, and to your responses to the questions at the end of this notebook.\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from math import exp, log # natural log, the log base doesn't matter, right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-11-5f9ff5a0f967>, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-5f9ff5a0f967>\"\u001b[1;36m, line \u001b[1;32m47\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\" Main function\n",
    "main(arg[]):\n",
    "\n",
    "Step 1 - clean data:\n",
    "    \n",
    "    # This function should return a data structure that contains clean data\n",
    "    \n",
    "    preprocess(String FILENAME){\n",
    "    \n",
    "        # read csv file from a string\n",
    "        \n",
    "          Strategy\n",
    "        # raw data handling 1. Missing value\n",
    "                            2. Outliers\n",
    "                            3. ?? \n",
    "        }\n",
    "# Don't worry about this part now , it should be step 5, Evaluation    \n",
    "Step 2 - Strategy for choosing test and train datasets                                                                                                                                                                                                                                                                                                                                                \n",
    "    \n",
    "    # there will be different functionality requires according to different strategy, often loops involved\n",
    "    # The point is to split dataset.\n",
    "#    \n",
    "    \n",
    "    \n",
    "    \n",
    "Step 3 - Building Bayse Model\n",
    "    \n",
    "    # This function will take a clean train dataset and return a model\n",
    "    \n",
    "    train(train_data): model\n",
    "    \n",
    "    # Update take an instance, and update the model, void function\n",
    "    update(model, instance)\n",
    "    \n",
    "Step 4 - Predict\n",
    "    \n",
    "    # This function will take an instance, then predict the label value with the higghest probability\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This function should open a data file in csv, and transform it into a usable format \n",
    "\n",
    "def readHeader(filename):\n",
    "    \n",
    "    # open header file\n",
    "    f = open(\"2019S1-proj1-data/headers.txt\", 'r') # This line is platform-specific\n",
    "    \n",
    "    # @flag that check if the file name matches the line in the header file\n",
    "    check_matched = False\n",
    "    # @flag if skip line \"=====\" in header.txt\n",
    "    check_skipped = False\n",
    "    \n",
    "    for line in f.readlines():\n",
    "        # check if filename match line\n",
    "        # get rid of \\n \n",
    "        if (line.strip('\\n') == (filename + '.csv')):\n",
    "            check_matched = True\n",
    "            continue\n",
    "        # change flags to skip line \"=========\" in header\n",
    "        if ((check_matched == True) and (check_skipped == False)):\n",
    "            check_skipped = True\n",
    "            continue\n",
    "        # split the header line and put it in a list\n",
    "        \n",
    "        if ((check_matched == True) and (check_skipped == True)):\n",
    "            # strip \\n\n",
    "            header = line.strip('\\n').split(\",\")\n",
    "            break\n",
    "    #close file\n",
    "    f.close()\n",
    "    \n",
    "    # return a list of string        \n",
    "    return header\n",
    "            \n",
    "            \n",
    "\n",
    "def preprocess(filename):\n",
    "    \n",
    "    # read header file\n",
    "    header = readHeader(filename)\n",
    "    \n",
    "    # read csv to pandas\n",
    "    df = pd.read_csv(\"2019S1-proj1-data/\" + filename +\".csv\", header=None, encoding = 'utf8')\n",
    "            \n",
    "    # set column names\n",
    "\n",
    "    df.columns = header\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = preprocess(\"anneal\")\n",
    "# a.head(10)\n",
    "# how you acess by columns\n",
    "#a.loc[:,'family']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model\n",
    "# the posteriors is list (attributes) of dicts (class labels) of dicts (attribute values) of counts\n",
    "\n",
    "# take input data\n",
    "# return a model (the posteriors is list (attributes) of dicts (class labels) of dicts (attribute values) of counts)\n",
    "# the model is a count\n",
    "def train(data):\n",
    "    # read data info\n",
    "    list_headerName = data.columns\n",
    "    n_attr = len(list_headerName)\n",
    "    # slice all the rows for the last column\n",
    "    column_class = data.iloc[:,-1]\n",
    "    # slice the rest of dataframe except the last column\n",
    "    df_classifier = data.illoc[:,:-2]\n",
    "    \n",
    "    # initialize model\n",
    "    \n",
    "    # the posteriors is list (attributes) of dicts (class labels) of \n",
    "    # dicts (attribute values) of counts\n",
    "    posteriors = [defaultdict(lambda: defaultdict(int)) for i in range(n_attr)]\n",
    "    # dictionary of class value count\n",
    "    priors = defaultdict(int)\n",
    "    # number of instance\n",
    "    n_inst = 0\n",
    "    \n",
    "    model = [posteriors, priors, n_inst]\n",
    "    \n",
    "    # update model with instance\n",
    "    \n",
    "    for row, new_label in zip(df_classifier.iterrows(), column_class):\n",
    "        model = update_model(model, row, new_label, list_headerName) \n",
    "    \n",
    "    \n",
    "def update_model(model, new_instanec, new_label, list_headerName):\n",
    "    \n",
    "    (posteriors, priors, n_inst) = model\n",
    "    \n",
    "    priors[new_label] += 1\n",
    "    \n",
    "    for attribute_value ,attribute_name in zip(row, list_headerName):\n",
    "        posteriors[attribute_name][new_label][attribute_value] += 1\n",
    "    \n",
    "\n",
    "    return (posteriors, priors, n_inst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\"    \n",
    "    for new_instance, new_label in zip(*data):\n",
    "        model = update_model(model, new_instance, new_label)\n",
    "    \n",
    "    return model\n",
    "\"\"\"\"\n",
    "\"\"\"\"    \n",
    "    for attribute_index, attribute_value in enumerate(new_instance):\n",
    "        posteriors[attribute_index][new_label][attribute_value] += 1\n",
    "    \n",
    "    n_inst += 1\n",
    "\"\"\"\"    \n",
    "\"\"\"\"\n",
    "def update_model(model, new_instance):\n",
    "    (posteriors, priors, n_inst) = model\n",
    "    \n",
    "    priors[new_label] += 1\n",
    "    \n",
    "    for attribute_index, attribute_value in enumerate(new_instance):\n",
    "        posteriors[attribute_index][new_label][attribute_value] += 1\n",
    "    \n",
    "    n_inst += 1\n",
    "    \n",
    "    return (posteriors, priors, n_inst)\n",
    "\"\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def log_prob(model, attribute_index, class_label, attribute_value):\n",
    "    \n",
    "    numerator = model[0][attribute_index][class_label][attribute_value]\n",
    "    denominator = model[1][class_label]\n",
    "    \n",
    "    if numerator == 0:\n",
    "        return 0 # <-- actually, should be infinity, fix later\n",
    "    else:\n",
    "        return log(numerator) - log(denominator)\n",
    "\n",
    "# find logP(class_label|instance)\n",
    "def class_likelihood(model, instance, class_label):\n",
    "    log_prior_prob = log(model[1][class_label]) - log(model[2])\n",
    "    log_post_probs = [log_prob(model, attribute_index, class_label, attribute_value)\n",
    "        for (attribute_index, attribute_value) in enumerate(instance)]\n",
    "    \n",
    "    return sum(log_post_probs) + log_prior_prob\n",
    "    \n",
    "# This function should predict the class for an instance or a set of instances, based on a trained model\n",
    "\n",
    "# take a model data structure and one instance\n",
    "# return a list of class value with probability\n",
    "def predict(model, instance):\n",
    "    class_labels = model[1].keys()\n",
    "    return zip(class_labels, [class_likelihood(model, instance, class_label) for class_label in class_labels])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('N', -1.5040773967762742), ('Y', -3.9889840465642745)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example from week 3 tutorial\n",
    "play = ([['s', 'h', 'n', 'F'],\n",
    "           ['s', 'h', 'h', 'T'],\n",
    "           ['o', 'h', 'h', 'F'],\n",
    "           ['r', 'm', 'h', 'F'],\n",
    "           ['r', 'c', 'n', 'F'],\n",
    "           ['r', 'c', 'n', 'T']],\n",
    "           ['N', 'N', 'Y', 'Y', 'Y', 'N'])\n",
    "\n",
    "play_model = train(play)\n",
    "list(predict(play_model, ['o', 'm', 'n', 'T']))\n",
    "# checked by hand, this is correct (if ignore zero probs) \n",
    "# [('N', -1.5040773967762742), ('Y', -3.9889840465642745)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This function should evaluate a set of predictions, in a supervised context \n",
    "def evaluate():\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This function should calculate the Information Gain of an attribute or a set of attribute, with respect to the class\n",
    "def info_gain():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Questions (you may respond in a cell or cells below):\n",
    "\n",
    "1. The Naive Bayes classifiers can be seen to vary, in terms of their effectiveness on the given datasets (e.g. in terms of Accuracy). Consider the Information Gain of each attribute, relative to the class distribution — does this help to explain the classifiers’ behaviour? Identify any results that are particularly surprising, and explain why they occur.\n",
    "2. The Information Gain can be seen as a kind of correlation coefficient between a pair of attributes: when the gain is low, the attribute values are uncorrelated; when the gain is high, the attribute values are correlated. In supervised ML, we typically calculate the Infomation Gain between a single attribute and the class, but it can be calculated for any pair of attributes. Using the pair-wise IG as a proxy for attribute interdependence, in which cases are our NB assumptions violated? Describe any evidence (or indeed, lack of evidence) that this is has some effect on the effectiveness of the NB classifier.\n",
    "3. Since we have gone to all of the effort of calculating Infomation Gain, we might as well use that as a criterion for building a “Decision Stump” (1-R classifier). How does the effectiveness of this classifier compare to Naive Bayes? Identify one or more cases where the effectiveness is notably different, and explain why.\n",
    "4. Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy. How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)\n",
    "5. Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the Naive Bayes classifier? Explain why, or why not.\n",
    "6. Naive Bayes is said to elegantly handle missing attribute values. For the datasets with missing values, is there any evidence that the performance is different on the instances with missing values, compared to the instances where all of the values are present? Does it matter which, or how many values are missing? Would a imputation strategy have any effect on this?\n",
    "\n",
    "Don't forget that groups of 1 student should respond to question (1), and one other question of your choosing. Groups of 2 students should respond to question (1) and question (2), and two other questions of your choosing. Your responses should be about 150-250 words each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
